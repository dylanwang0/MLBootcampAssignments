{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week4Assignment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhuymJldwQq8"
      },
      "source": [
        "## MLBootcamp21 Week 4 Assignment: Gridsearch and CrossValidation\n",
        "- This week we will focus on improving our existing models using these two techniques\n",
        "- They are good approaches to improving your model's performance apart from feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtQ2vhXkwptR"
      },
      "source": [
        "## Q1. Use GridSearchCV and also explore other GridSearch techniques provided by scikit-learn to tune your hyperparameters and obtain the best model\n",
        "\n",
        "- Try out Decision Trees, Random Forest, GradientBoost, AdaBoost and also XGBoost \n",
        "- Run the model on the features you have generated until now without tuning the parameters first to check the result on basic parameters\n",
        "- Then apply GridSearchCV or other GridSearch techniques provided by scikit-learn to tune the hyperparameters and get results on the best model\n",
        "\n",
        "## Keep the \"random_state\" number as 42 or anynumber of your choice and report that number for me to be able to reproduce the same results\n",
        "\n",
        "- Report your performance on the test set after making the submission on kaggle. \n",
        "\n",
        "- ****Do not use some random existing notebook on Kaggle to get the best results as you will not learn anything that way and we will be able to easily know if that has been done. Do whatever you can****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmFiP4zSwo5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b49248-b77a-4279-da6c-ccfe1825ebbd"
      },
      "source": [
        "# Write your code from this cell\n",
        "# It need not be in a single cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "df_train = pd.read_csv(\"./train.csv\")\n",
        "\n",
        "df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace = True)\n",
        "df_train['Fare'].fillna(df_train['Fare'].median(), inplace = True)\n",
        "df_train['Age'].fillna(df_train['Age'].median(),inplace=True)\n",
        "\n",
        "df_train[['female','male']] = pd.get_dummies(df_train['Sex'])\n",
        "df_train[[\"C\",\"Q\",\"S\"]] = pd.get_dummies(df_train[\"Embarked\"])\n",
        "df_train.fillna(method=\"ffill\",inplace=True)\n",
        "drop_features = [\"Sex\",'Ticket','Name','Cabin',\"Embarked\",'female']\n",
        "df_train.drop(drop_features,inplace=True,axis=1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_train.loc[:,'Pclass':],df_train.Survived,random_state=42,\\\n",
        "                                                    test_size=0.2)\n",
        "#Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(x_train,y_train)\n",
        "dt_model_predictions = dt_model.predict(x_test)\n",
        "print(\"Decision Tree Orginal\")\n",
        "print(classification_report(y_test,dt_model_predictions))\n",
        "\n",
        "#Decision Tree GridSearch CV\n",
        "parameter_grid = {\n",
        "    'criterion' : ['gini','entropy'],\n",
        "    'max_leaf_nodes' : [2,4,5,7,10,15,17,20],\n",
        "    'max_features' : ['auto','log2','sqrt'],\n",
        "}\n",
        "\n",
        "dt_model_gridsearched = GridSearchCV(cv=5,estimator=DecisionTreeClassifier(random_state=42), param_grid = parameter_grid)\n",
        "dt_model_gridsearched.fit(x_train,y_train)\n",
        "print(dt_model_gridsearched.best_params_)\n",
        "print(\"Decision Tree GridSearched\")\n",
        "print(classification_report(y_test,dt_model_gridsearched.predict(x_test)))\n",
        "\n",
        "#Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42,bootstrap=False,criterion='entropy',max_features='log2',max_leaf_nodes=10,n_estimators=100)\n",
        "rf_model.fit(x_train,y_train)\n",
        "rf_model_predictions = rf_model.predict(x_test)\n",
        "print(\"Random Forest Orginal\")\n",
        "print(classification_report(y_test,rf_model_predictions))\n",
        "\n",
        "#Random Forest GridSearch CV\n",
        "parameter_grid = {\n",
        "    'bootstrap' : [True,False],\n",
        "    'criterion' : ['gini','entropy'],\n",
        "    'n_estimators' : [10,20,50,100],\n",
        "    'max_leaf_nodes' : [2,4,5,7,10],\n",
        "    'max_features' : ['auto','log2','sqrt']\n",
        "}\n",
        "\n",
        "rf_model_gridsearched = GridSearchCV(cv=5,estimator=RandomForestClassifier(random_state=42), param_grid = parameter_grid)\n",
        "rf_model_gridsearched.fit(x_train,y_train)\n",
        "print(rf_model_gridsearched.best_params_)\n",
        "print(\"Random Forest GridSearched\")\n",
        "print(classification_report(y_test,rf_model_gridsearched.predict(x_test)))\n",
        "\n",
        "#Gradient Boost\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(x_train,y_train)\n",
        "gb_model_predictions = gb_model.predict(x_test)\n",
        "print(\"Gradient Boost Original\")\n",
        "print(classification_report(y_test,gb_model_predictions))\n",
        "\n",
        "#Gradient Boost GridSearch CV\n",
        "parameter_grid = {\n",
        "    'loss' : ['deviance','exponential'],\n",
        "    'learning_rate' : [0.1,0.3,0.5,0.7,0.9],\n",
        "    'n_estimators' : [10,20,50,100],\n",
        "    'max_leaf_nodes' : [2,4,5,7,10],\n",
        "}\n",
        "\n",
        "gb_model_gridsearched = GridSearchCV(cv=5,estimator=GradientBoostingClassifier(random_state=42), param_grid = parameter_grid)\n",
        "gb_model_gridsearched.fit(x_train,y_train)\n",
        "print(gb_model_gridsearched.best_params_)\n",
        "print(\"Gradient Boost GridSearched\")\n",
        "print(classification_report(y_test,gb_model_gridsearched.predict(x_test)))\n",
        "\n",
        "#AdaBoost\n",
        "ab_model = AdaBoostClassifier(random_state=42)\n",
        "ab_model.fit(x_train,y_train)\n",
        "ab_model_predictions = ab_model.predict(x_test)\n",
        "print(\"AdaBoost Original\")\n",
        "print(classification_report(y_test,ab_model_predictions))\n",
        "\n",
        "#AdaBoost GridSearch CV\n",
        "parameter_grid = {\n",
        "    'learning_rate' : [0.1,0.3,0.5,0.7,0.9],\n",
        "    'n_estimators' : [10,20,50,100],\n",
        "    'algorithm' : ['SAMME','SAMME.R']\n",
        "}\n",
        "\n",
        "ab_model_gridsearched = GridSearchCV(cv=5,estimator=AdaBoostClassifier(random_state=42), param_grid = parameter_grid)\n",
        "ab_model_gridsearched.fit(x_train,y_train)\n",
        "print(ab_model_gridsearched.best_params_)\n",
        "print(\"AdaBoost GridSearched\")\n",
        "print(classification_report(y_test,ab_model_gridsearched.predict(x_test)))\n",
        "\n",
        "#XGBoost\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "xgb_model.fit(x_train,y_train)\n",
        "xgb_model_predictions = xgb_model.predict(x_test)\n",
        "print(\"XGBoost Original\")\n",
        "print(classification_report(y_test,xgb_model_predictions))\n",
        "\n",
        "#XGBoost GridSearch CV\n",
        "parameter_grid = {\n",
        "    'learning_rate' : [0.1,0.3,0.5,0.7,0.9],\n",
        "    'n_estimators' : [10,20,50,100],\n",
        "    'booster' : ['gbtree','gblinear','dart'],\n",
        "    'max_depth' : [1,3,5,7]\n",
        "}\n",
        "\n",
        "xgb_model_gridsearched = GridSearchCV(cv=5,estimator=XGBClassifier(random_state=42), param_grid = parameter_grid)\n",
        "xgb_model_gridsearched.fit(x_train,y_train)\n",
        "print(xgb_model_gridsearched.best_params_)\n",
        "print(\"XGBoost GridSearched\")\n",
        "print(classification_report(y_test,xgb_model_gridsearched.predict(x_test)))\n",
        "\n",
        "#Test Submission\n",
        "df_test = pd.read_csv(\"./test.csv\")\n",
        "\n",
        "df_test['Embarked'].fillna(df_test['Embarked'].mode()[0], inplace = True)\n",
        "df_test['Fare'].fillna(df_test['Fare'].median(), inplace = True)\n",
        "df_test['Age'].fillna(df_test['Age'].median(),inplace=True)\n",
        "\n",
        "df_test[['female','male']] = pd.get_dummies(df_test['Sex'])\n",
        "df_test[[\"C\",\"Q\",\"S\"]] = pd.get_dummies(df_test[\"Embarked\"])\n",
        "df_test.fillna(method=\"ffill\",inplace=True)\n",
        "drop_features = [\"Sex\",'Ticket','Name','Cabin',\"Embarked\",'female']\n",
        "df_test.drop(drop_features,inplace=True,axis=1)\n",
        "\n",
        "#Gradient Boosting got the best results\n",
        "predictions_for_submission = gb_model_gridsearched.predict(df_test.loc[:,'Pclass':])\n",
        "df_submission = df_test[['PassengerId']].copy()\n",
        "df_submission['Survived'] = predictions_for_submission\n",
        "df_submission.to_csv(\"submission_v1.csv\")\n",
        "\n",
        "#Kaggle Submission\n",
        "results = 0.79425\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree Orginal\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       105\n",
            "           1       0.72      0.78      0.75        74\n",
            "\n",
            "    accuracy                           0.78       179\n",
            "   macro avg       0.78      0.78      0.78       179\n",
            "weighted avg       0.79      0.78      0.78       179\n",
            "\n",
            "Random Forest Orginal\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.91      0.85       105\n",
            "           1       0.84      0.66      0.74        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.82      0.79      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "Gradient Boost Original\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.85       105\n",
            "           1       0.83      0.72      0.77        74\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.82      0.81      0.81       179\n",
            "weighted avg       0.82      0.82      0.82       179\n",
            "\n",
            "{'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 7, 'max_leaf_nodes': 10, 'n_estimators': 50}\n",
            "Gradient Boost GridSearched\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.85       105\n",
            "           1       0.83      0.72      0.77        74\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.82      0.81      0.81       179\n",
            "weighted avg       0.82      0.82      0.82       179\n",
            "\n",
            "AdaBoost Original\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83       105\n",
            "           1       0.77      0.76      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.80      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n",
            "XGBoost Original\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85       105\n",
            "           1       0.82      0.69      0.75        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.79      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfx1tUtmykXK"
      },
      "source": [
        "## Q2. There might be times when you would only like to do K-Fold Cross Validation and not run the time consuming GridSearch everytime. That is what you will be doing in this question. \n",
        "\n",
        "- Read the documention for K-Fold crossvalidation provided by scikit-learn here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "- Try to understand and run the code provided by scikit-learn. Basically this function provided by scikit-learn will split your dataset into the \"K\" folds and will give you the indices of the \"K\" folds\n",
        "\n",
        "- Then use the K-Fold crossvalidation technique to generate the \"K\" folds and report the accuracy obtained for each of the \"K\" folds\n",
        "\n",
        "- You should get \"K\" different accuracy values and then finally take the average of all the \"K\" accuracies which would be your final model performance\n",
        "\n",
        "- This exercise will help you really understand K-Fold CrossValidation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp7w1n-DyjpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "471d744b-629d-40ad-a7a2-0e2fed7885f8"
      },
      "source": [
        "## Write your code from here\n",
        "kf = KFold(n_splits=10,shuffle=True,random_state=42)\n",
        "clf = DecisionTreeClassifier(random_state=42,criterion='gini',max_features='auto',max_leaf_nodes=20)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "print(scores.mean())\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42,bootstrap=True,criterion='entropy',max_features='auto',max_leaf_nodes=10,n_estimators=10)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "print(scores.mean())\n",
        "\n",
        "clf = GradientBoostingClassifier(random_state=42,learning_rate=0.5,loss='exponential',max_leaf_nodes=7,n_estimators=20)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "print(scores.mean())\n",
        "\n",
        "clf = AdaBoostClassifier(random_state=42,algorithm='SAMME.R',learning_rate=0.9,n_estimators=100)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "print(scores.mean())\n",
        "\n",
        "clf = XGBClassifier(random_state=42,booster='gbtree',learning_rate=0.3,n_estimators=20)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
        "print(scores.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8118290160543682\n",
            "0.832847434255885\n",
            "0.8328474342558849\n",
            "0.8103713188220232\n",
            "0.8328474342558849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP1FogY_zrTk"
      },
      "source": [
        "## Thats it for this week. There are only two questions but they are a bit time consuming so enjoy! I hope you reach atleast 85-90% on Kaggle before tapping into Deep Learning"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}